{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "06f22e03-17d2-45c2-95a4-b617e45a6339"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i9LXloxlIWr"
      },
      "source": [
        "### 1. Macro-averaged F1-score\n",
        "\n",
        "Implement the `macro_averaged_f1_score()` function which computes the Macro-averaged F1 score for a multi-class classification problem.\n",
        "\n",
        "\n",
        "$$\\text{Precision (P) = }\\frac{TP}{TP+FP}$$\n",
        "<br>\n",
        "$$\\text{Recall (R) = }\\frac{TP}{TP+FN}$$\n",
        "<br>\n",
        "$$\\text{F1-score = }\\frac{2PR}{P+R}$$\n",
        "<br>\n",
        "$$\\text{Macro-averaged F1-score = }\\frac{\\text{Sum of F1-scores of all classes }}{\\text{Number of classes}}$$\n",
        "\n",
        "\n",
        "**Arguments**:\n",
        "* **`actual_Y`** :  Actual labels of instances.\n",
        "    * A 1D numpy array of chars\n",
        "    * Array shape: (number of instances, )\n",
        "* **`predicted_Y`**: Predicted labels of instances.\n",
        "    * A 1D numpy array of chars\n",
        "    * Array shape: (number of instances, )\n",
        "    * Assume that `predicted_Y` does not have labels which are not in `actual_Y`\n",
        "\n",
        "\n",
        "**Returns**:\n",
        "Macro-averaged F1-score (A float value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03aeced1-552d-46f5-910b-9f10ff01836f"
      },
      "source": [
        "def macro_averaged_f1_score(actual_Y, predicted_Y):\n",
        "  labels=np.unique(actual_Y)\n",
        "  total_f1_score=0.0\n",
        "  for i in labels:\n",
        "    actual_positives=np.count_nonzero(actual_Y==i)\n",
        "    predicted_positives=np.count_nonzero(predicted_Y==i)\n",
        "    true_positives=np.logical_and(actual_Y==i,predicted_Y==i)\n",
        "    true_positives_count=np.count_nonzero(true_positives)\n",
        "    recall=0.0\n",
        "    precision=0.0\n",
        "    f1_score=0.0\n",
        "    if (true_positives_count!=0):\n",
        "      recall=true_positives_count/actual_positives\n",
        "      precision=true_positives_count/predicted_positives\n",
        "      f1_score=(2*recall*precision)/(recall+precision)\n",
        "    total_f1_score+=f1_score\n",
        "  return total_f1_score/labels.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero?"
      ],
      "metadata": {
        "id": "wy6SV9MB1nw6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15FKlXP0n8MK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fafa6a0-6853-443c-f0d9-5612b54d2024"
      },
      "source": [
        "actual_Y = np.array([\"A\",\"A\",\"B\",\"C\",\"C\"])\n",
        "predicted_Y = np.array([\"A\",\"B\",\"B\",\"C\",\"B\"])\n",
        "f1_score = macro_averaged_f1_score(actual_Y, predicted_Y)\n",
        "print(np.round(f1_score, 3))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL-PvZB-pKpn"
      },
      "source": [
        "**Expected Output**:\n",
        "```\n",
        "0.611\n",
        "```"
      ]
    }
  ]
}